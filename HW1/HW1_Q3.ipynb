{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display:block;width:100%;margin:auto;\" direction=rtl align=center><br><br>\n",
        "    <div  style=\"width:100%;margin:100;display:block;background-color:#fff0;\"  display=block align=center>\n",
        "        <table style=\"border-style:hidden;border-collapse:collapse;\">\n",
        "            <tr>\n",
        "                <td  style=\"border: none!important;\">\n",
        "        <img width=130 align=right src=\"https://i.ibb.co/yXKQmtZ/logo1.png\" style=\"margin:0;\" />\n",
        "                </td>\n",
        "                <td style=\"text-align:center;border: none!important;\">\n",
        "                <h1 align=center><font size=5 color=\"#045F5F\"> <b>Deep Learning (Fall 2023)</b><br><br><b>Homework-1</i><br><br><h1><h3>Clustering with Autoencoer-Q3</i></font></h3>\n",
        "                </td>\n",
        "                <td style=\"border: none!important;\">\n",
        "        <img width=170 align=left  src=\"https://i.ibb.co/wLjqFkw/logo2.png\" style=\"margin:0;\" />\n",
        "                </td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    \n",
        "</div>\n",
        "<br>\n",
        "<font size=3 color=\"#045F5F\">Mohammad Javad Ranjbar<br></font><br>\n",
        "<font size=3 color=\"#045F5F\">Behrad Mosaie<br></font><br>"
      ],
      "metadata": {
        "id": "IY_YfAReazPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Libraries\n",
        "In this code cell, we set up the necessary libraries and dependencies for knowledge distillation using a neural network. We import key libraries, including PyTorch, torchvision, and additional modules for data transformation, visualization, and progress tracking. Make sure to run this cell before proceeding with the rest of the code.\n",
        "\n"
      ],
      "metadata": {
        "id": "gwME7HpSa8WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import davies_bouldin_score\n"
      ],
      "metadata": {
        "id": "Dzpvwjz1UYQv"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. DAC\n",
        "The code defines a Deep Autoencoder Clustering (DAC) model using TensorFlow and Keras. DAC is a neural network architecture used for unsupervised clustering of data, often used in applications like image compression and feature learning. This model is designed to perform clustering on data with an encoder-decoder architecture.\n",
        "\n",
        "Model Components:\n",
        "\n",
        "DAC Class:\n",
        " he DAC class is defined as a subclass of tf.keras.Model. It takes three main parameters:\n",
        "\n",
        "input_dim: The dimension of the input data.\n",
        "latent_dim: The dimension of the latent space (bottleneck) where data is compressed.\n",
        "num_clusters: The number of clusters, which corresponds to the output dimension of the last layer.\n",
        "Encoder: The encoder is defined as a sequence of dense layers that gradually reduce the input dimension:\n",
        "\n",
        "The first layer has 512 units and uses ReLU activation.\n",
        "The second layer has 128 units with ReLU activation.\n",
        "The third layer has 32 units with hyperbolic tangent (tanh) activation.\n",
        "The final layer has 10 units with tanh activation.\n",
        "Decoder: The decoder is defined as a sequence of dense layers that aim to reconstruct the input data from the compressed representation:\n",
        "\n",
        "The first layer has 32 units with tanh activation.\n",
        "The second layer has 128 units with tanh activation.\n",
        "The third layer has 512 units with tanh activation.\n",
        "The final layer has latent_dim units, which is the dimension of the latent space."
      ],
      "metadata": {
        "id": "OziOj2fibAh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "43GGqhRK158V"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DAC(tf.keras.Model):\n",
        "    def __init__(self, input_dim, latent_dim, num_clusters):\n",
        "        super(DAC, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(512, activation='relu', input_shape=(input_dim,)),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(32, activation='tanh'),\n",
        "            tf.keras.layers.Dense(10, activation='tanh')\n",
        "        ])\n",
        "        # Decoder\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(32, activation='tanh', input_shape=(10,)),\n",
        "            tf.keras.layers.Dense(128, activation='tanh'),\n",
        "            tf.keras.layers.Dense(512, activation='tanh'),\n",
        "            tf.keras.layers.Dense(latent_dim)\n",
        "        ])\n",
        "        # Clustering layer\n",
        "        self.sigmoid1 = tf.keras.layers.Activation('sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        # Encoder\n",
        "        encoded = self.encoder(x)\n",
        "        # Decoder\n",
        "        decoded = self.decoder(encoded)\n",
        "        # Clustering\n",
        "        outputs = self.sigmoid1(decoded)\n",
        "        return outputs\n",
        "\n",
        "# Example usage:\n",
        "input_dim = 784  # For MNIST\n",
        "latent_dim = 32  # You can choose an appropriate latent dimension\n",
        "num_clusters = 10  # Number of clusters (output dimension of the last layer)\n",
        "\n",
        "model = DAC(input_dim, latent_dim, num_clusters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight\n",
        "\n",
        "The code defines a function, compute_w, which calculates a weight vector w for individual feature dimensions. These weights indicate the importance of each feature in distinguishing different clusters within a dataset. The computation method considers both the similarity of feature values within the same cluster and the dissimilarity between feature values in different clusters."
      ],
      "metadata": {
        "id": "cNDgHRm-gMGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_feature_weights(features, label, num_samples):\n",
        "    num_features = features.shape[1]\n",
        "    w = np.zeros((num_features,))\n",
        "\n",
        "    for i in range(num_features):\n",
        "        for j in range(num_samples):\n",
        "            for k in range(num_samples):\n",
        "                if label[j] == label[k]:\n",
        "                    w[i] += np.exp(-((features[j, i] - features[k, i]) ** 2))\n",
        "                else:\n",
        "                    w[i] -= np.exp(-((features[j, i] - features[k, i]) ** 2))\n",
        "    w = preprocessing.normalize([w])\n",
        "    return w"
      ],
      "metadata": {
        "id": "mLCalldc5z0O"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "\n",
        "\n",
        "# Now, `random_x_train` contains 1000 random samples from the original `x_train` dataset.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "# Reshape the data for the model\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "RDHqol-7qk24"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_feature_weights(data, label):\n",
        "    num_sampels=data.shape[0]\n",
        "    num_features=data.shape[1]\n",
        "    w = np.zeros(num_features)\n",
        "\n",
        "    for i in range(num_features):\n",
        "      for j in range(num_sampels):\n",
        "        for k in range(num_sampels):\n",
        "          if label[j] == label[k]:\n",
        "            w[i] += np.exp(-(data[j,i]-data[k,i])**2)\n",
        "          else:\n",
        "            w[i] += 1 - np.exp(-(data[j,i]-data[k,i])**2)\n",
        "\n",
        "    min_weight = np.min(w)\n",
        "    max_weight = np.max(w)\n",
        "    w = (w - min_weight) / (max_weight - min_weight)\n",
        "    return w"
      ],
      "metadata": {
        "id": "UylPIV4Qvj25"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = x_train.shape[1]\n",
        "# Shuffle the training data\n",
        "shuffled_indices = np.arange(x_train.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "# Get the first 500 samples (not 1000 as you mentioned in the comment)\n",
        "num_samples = 100\n",
        "random_x_train = x_train[shuffled_indices[:num_samples]]\n",
        "random_y_train = y_train[shuffled_indices[:num_samples]]\n",
        "\n",
        "# Calculate the weights\n",
        "weights = compute_feature_weights(random_x_train, random_y_train)\n"
      ],
      "metadata": {
        "id": "eugEZmpB6EwI"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this heatmap, we visually represent the importance of features in a dataset of 500 samples. Notably, features located closer to the center of the image hold greater significance, aligning with the centrality of features in MNIST data."
      ],
      "metadata": {
        "id": "97Ux2OqQh4WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_array = np.array(weights)\n",
        "output_array = weights.reshape((28, 28))\n",
        "plt.imshow(output_array, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "dnui6fgP6555",
        "outputId": "34cb6004-556f-4d74-ec17-9ef692f8b465"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff68887e770>"
            ]
          },
          "metadata": {},
          "execution_count": 136
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python function, clustering_weighted_mse_loss, calculates the weighted mean squared error (MSE) loss between the true values (y_true) and the predicted values (y_pred) using a set of weights (weights) that considers the most important feautres."
      ],
      "metadata": {
        "id": "H1hwqPcQllMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering_weighted_mse_loss(y_true, y_pred,weights):\n",
        "  loss = weights*((y_true - y_pred)**2)\n",
        "  return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "nRbAX9Qe8H1G"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes a DAC (Deep Autoencoder for Clustering) model with specific dimensions and number of clusters. It uses a custom loss function and the Adam optimizer for training. The training loop runs for a specified number of epochs, where data is divided into batches for training, and model weights are updated accordingly, with the loss printed at each epoch.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s8NYO0KDmEMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the DAC model\n",
        "input_dim = x_train.shape[1]\n",
        "latent_dim = 784  # Choose an appropriate latent dimension\n",
        "num_clusters = 10\n",
        "model = DAC(input_dim, latent_dim, num_clusters)\n",
        "\n",
        "# Define the loss function (e.g., mean squared error)\n",
        "loss_fn = clustering_weighted_mse_loss\n",
        "\n",
        "# Define the optimizer (e.g., Adam)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4,weight_decay=0.00001)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch = x_train[i:i+batch_size]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            cluster_output = model(x_batch)\n",
        "            loss = loss_fn(x_batch, cluster_output,weights)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZpk2pGg3Dix",
        "outputId": "47aab3a0-1e0c-4537-fa26-7bd2359d045a"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.024853438138961792\n",
            "Epoch 2, Loss: 0.01832585409283638\n",
            "Epoch 3, Loss: 0.015772975981235504\n",
            "Epoch 4, Loss: 0.014166552573442459\n",
            "Epoch 5, Loss: 0.013006295077502728\n",
            "Epoch 6, Loss: 0.01217908039689064\n",
            "Epoch 7, Loss: 0.011603920720517635\n",
            "Epoch 8, Loss: 0.01111545693129301\n",
            "Epoch 9, Loss: 0.010699561797082424\n",
            "Epoch 10, Loss: 0.010342519730329514\n",
            "Epoch 11, Loss: 0.009995510801672935\n",
            "Epoch 12, Loss: 0.009685509838163853\n",
            "Epoch 13, Loss: 0.009424980729818344\n",
            "Epoch 14, Loss: 0.009094811975955963\n",
            "Epoch 15, Loss: 0.00883902981877327\n",
            "Epoch 16, Loss: 0.008598150685429573\n",
            "Epoch 17, Loss: 0.008371972478926182\n",
            "Epoch 18, Loss: 0.008186965249478817\n",
            "Epoch 19, Loss: 0.008028550073504448\n",
            "Epoch 20, Loss: 0.007914967834949493\n",
            "Epoch 21, Loss: 0.007810044102370739\n",
            "Epoch 22, Loss: 0.007694763597100973\n",
            "Epoch 23, Loss: 0.0075557418167591095\n",
            "Epoch 24, Loss: 0.00740518793463707\n",
            "Epoch 25, Loss: 0.007321713957935572\n",
            "Epoch 26, Loss: 0.007210373878479004\n",
            "Epoch 27, Loss: 0.007137961685657501\n",
            "Epoch 28, Loss: 0.007011248730123043\n",
            "Epoch 29, Loss: 0.006898884661495686\n",
            "Epoch 30, Loss: 0.006771169602870941\n",
            "Epoch 31, Loss: 0.006671076640486717\n",
            "Epoch 32, Loss: 0.006566910073161125\n",
            "Epoch 33, Loss: 0.006494053173810244\n",
            "Epoch 34, Loss: 0.006426721811294556\n",
            "Epoch 35, Loss: 0.006360234227031469\n",
            "Epoch 36, Loss: 0.006292305421084166\n",
            "Epoch 37, Loss: 0.006225926335901022\n",
            "Epoch 38, Loss: 0.006159575656056404\n",
            "Epoch 39, Loss: 0.006140564102679491\n",
            "Epoch 40, Loss: 0.006292583886533976\n",
            "Epoch 41, Loss: 0.006425387226045132\n",
            "Epoch 42, Loss: 0.00630317535251379\n",
            "Epoch 43, Loss: 0.006154173519462347\n",
            "Epoch 44, Loss: 0.005986104719340801\n",
            "Epoch 45, Loss: 0.005885585211217403\n",
            "Epoch 46, Loss: 0.005806969944387674\n",
            "Epoch 47, Loss: 0.005750634241849184\n",
            "Epoch 48, Loss: 0.005737269297242165\n",
            "Epoch 49, Loss: 0.005690739490091801\n",
            "Epoch 50, Loss: 0.005681370384991169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, you can use the encoder part of the model to cluster data\n",
        "num_clusters=10\n",
        "encoder_output = model.encoder(x_test)\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(encoder_output)\n",
        "predicted_labels = kmeans.labels_\n",
        "ari = adjusted_rand_score(y_test, predicted_labels)\n",
        "print(f\"Adjusted Rand Index: {ari}\")\n"
      ],
      "metadata": {
        "id": "zosC_2PZ5rTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367332de-d2d1-4104-f5f5-ba4a5a1c63df"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index: 0.5689845557630019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters=10\n",
        "encoder_output = model.encoder(x_train)\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(encoder_output)\n",
        "predicted_labels = kmeans.labels_\n",
        "\n",
        "ari = adjusted_rand_score(y_train, predicted_labels)\n",
        "print(f\"Adjusted Rand Index: {ari}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bdZZ1LmLn4Y",
        "outputId": "425d512f-1854-440a-8627-c1f1944a5c47"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index: 0.5603453542532038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters=10\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(x_train)\n",
        "predicted_labels = kmeans.labels_\n",
        "\n",
        "ari = adjusted_rand_score(y_train, predicted_labels)\n",
        "print(f\"Adjusted Rand Index: {ari}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCBCNpS_Lqaj",
        "outputId": "a508eb5d-0fc5-4287-8fb5-efb08e5c4107"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index: 0.3607179625149027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters=10\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(x_test)\n",
        "predicted_labels = kmeans.labels_\n",
        "\n",
        "ari = adjusted_rand_score(y_test, predicted_labels)\n",
        "print(f\"Adjusted Rand Index: {ari}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spjTmF4cLtZ5",
        "outputId": "5ce1d437-e181-4b7c-d80a-72f35e3e247c"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index: 0.38099932644261486\n"
          ]
        }
      ]
    }
  ]
}